{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "626a5698",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "23/04/30 03:12:13 WARN org.apache.spark.sql.streaming.StreamingQueryManager: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-6267ea50-32b1-4571-b278-d7270742c9a6. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n23/04/30 03:12:13 WARN org.apache.spark.sql.streaming.StreamingQueryManager: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n                                                                                \r"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "-------------------------------------------\nBatch: 0\n-------------------------------------------\n+------------------------------------------+------------------------------------------------------------------------------+-----------+\n|window                                    |location                                                                      |total_likes|\n+------------------------------------------+------------------------------------------------------------------------------+-----------+\n|{2023-04-17 17:00:00, 2023-04-17 18:00:00}| so we'll monitor him                                                         |null       |\n|{2023-04-20 11:00:00, 2023-04-20 12:00:00}|No location available.                                                        |25         |\n|{2023-04-12 23:30:00, 2023-04-13 00:30:00}|null                                                                          |null       |\n|{2023-04-21 22:30:00, 2023-04-21 23:30:00}|null                                                                          |null       |\n|{2023-04-15 20:00:00, 2023-04-15 21:00:00}| or us making open shots?\"\"\"                                                  |null       |\n|{2023-04-16 04:30:00, 2023-04-16 05:30:00}|No location available.                                                        |85         |\n|{2023-04-21 09:30:00, 2023-04-21 10:30:00}| try to take me away and it'll be his turn to do it. That's just how we playâ€¦\"|null       |\n|{2023-04-14 23:00:00, 2023-04-15 00:00:00}|Illinois, USA, United States                                                  |1          |\n|{2023-04-15 16:30:00, 2023-04-15 17:30:00}|Elancourt, France, France                                                     |2          |\n|{2023-04-20 17:00:00, 2023-04-20 18:00:00}|null                                                                          |null       |\n|{2023-04-15 21:30:00, 2023-04-15 22:30:00}|Jersey City, NJ, United States                                                |0          |\n|{2023-04-17 21:00:00, 2023-04-17 22:00:00}|null                                                                          |null       |\n|{2023-04-18 23:30:00, 2023-04-19 00:30:00}|Manhattan, NY, United States                                                  |1          |\n|{2023-04-17 03:30:00, 2023-04-17 04:30:00}|Minneapolis, MN, United States                                                |4          |\n|{2023-04-20 19:30:00, 2023-04-20 20:30:00}|null                                                                          |null       |\n|{2023-04-20 02:00:00, 2023-04-20 03:00:00}|Toronto, Ontario, Canada                                                      |0          |\n|{2023-04-19 00:30:00, 2023-04-19 01:30:00}|Manhattan, NY, United States                                                  |0          |\n|{2023-04-17 16:00:00, 2023-04-17 17:00:00}|Bronx, NY, United States                                                      |0          |\n|{2023-04-21 20:30:00, 2023-04-21 21:30:00}|null                                                                          |null       |\n|{2023-04-17 22:30:00, 2023-04-17 23:30:00}|Arkansas, USA, United States                                                  |0          |\n+------------------------------------------+------------------------------------------------------------------------------+-----------+\nonly showing top 20 rows\n\n"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[3], line 74\u001B[0m\n\u001B[1;32m     63\u001B[0m \u001B[38;5;66;03m# Start the streaming query and continuously output results to the console\u001B[39;00m\n\u001B[1;32m     64\u001B[0m query \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m     65\u001B[0m     likes_by_location_windowed\n\u001B[1;32m     66\u001B[0m     \u001B[38;5;241m.\u001B[39mwriteStream\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     71\u001B[0m     \u001B[38;5;241m.\u001B[39mstart()\n\u001B[1;32m     72\u001B[0m )\n\u001B[0;32m---> 74\u001B[0m \u001B[43mquery\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mawaitTermination\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/usr/lib/spark/python/pyspark/sql/streaming.py:101\u001B[0m, in \u001B[0;36mStreamingQuery.awaitTermination\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m     99\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jsq\u001B[38;5;241m.\u001B[39mawaitTermination(\u001B[38;5;28mint\u001B[39m(timeout \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m1000\u001B[39m))\n\u001B[1;32m    100\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 101\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jsq\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mawaitTermination\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py:1303\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1296\u001B[0m args_command, temp_args \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_build_args(\u001B[38;5;241m*\u001B[39margs)\n\u001B[1;32m   1298\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1299\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1300\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1301\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[0;32m-> 1303\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend_command\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcommand\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1304\u001B[0m return_value \u001B[38;5;241m=\u001B[39m get_return_value(\n\u001B[1;32m   1305\u001B[0m     answer, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_id, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname)\n\u001B[1;32m   1307\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
      "File \u001B[0;32m/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py:1033\u001B[0m, in \u001B[0;36mGatewayClient.send_command\u001B[0;34m(self, command, retry, binary)\u001B[0m\n\u001B[1;32m   1031\u001B[0m connection \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_connection()\n\u001B[1;32m   1032\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1033\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[43mconnection\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend_command\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcommand\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1034\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m binary:\n\u001B[1;32m   1035\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m response, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_create_connection_guard(connection)\n",
      "File \u001B[0;32m/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py:1200\u001B[0m, in \u001B[0;36mGatewayConnection.send_command\u001B[0;34m(self, command)\u001B[0m\n\u001B[1;32m   1196\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JNetworkError(\n\u001B[1;32m   1197\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mError while sending\u001B[39m\u001B[38;5;124m\"\u001B[39m, e, proto\u001B[38;5;241m.\u001B[39mERROR_ON_SEND)\n\u001B[1;32m   1199\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1200\u001B[0m     answer \u001B[38;5;241m=\u001B[39m smart_decode(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreadline\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m[:\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m])\n\u001B[1;32m   1201\u001B[0m     logger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAnswer received: \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(answer))\n\u001B[1;32m   1202\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m answer\u001B[38;5;241m.\u001B[39mstartswith(proto\u001B[38;5;241m.\u001B[39mRETURN_MESSAGE):\n",
      "File \u001B[0;32m/opt/conda/miniconda3/lib/python3.8/socket.py:669\u001B[0m, in \u001B[0;36mSocketIO.readinto\u001B[0;34m(self, b)\u001B[0m\n\u001B[1;32m    667\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[1;32m    668\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 669\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_sock\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrecv_into\u001B[49m\u001B[43m(\u001B[49m\u001B[43mb\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    670\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m timeout:\n\u001B[1;32m    671\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_timeout_occurred \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType, IntegerType, BooleanType\n",
    "from pyspark.sql.functions import col, lit, sum, window, udf\n",
    "import time\n",
    "\n",
    "# create SparkSession\n",
    "spark = SparkSession.builder.appName(\"myApp\").getOrCreate()\n",
    "\n",
    "# define the schema for the DataFrame\n",
    "schema = StructType([\n",
    "    StructField(\"team\", StringType(), True),\n",
    "    StructField(\"timestamp\", TimestampType(), True),\n",
    "    StructField(\"text\", StringType(), True),\n",
    "    StructField(\"location\", StringType(), True),\n",
    "    StructField(\"likes\", IntegerType(), True),\n",
    "    StructField(\"sentiment\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "#use gs link\n",
    "csvpath = \"gs://dataproc-staging-us-central1-300388429198-ocbfs3ky/inputdata/\"\n",
    "\n",
    "# read CSV file into streaming DataFrame with specified schema\n",
    "df = (\n",
    "    spark.readStream\n",
    "    .schema(schema)\n",
    "    .option(\"header\", \"false\")\n",
    "    .csv(csvpath)\n",
    ")\n",
    "\n",
    "# Perform data preprocessing steps on streaming_df (similar to the original code)\n",
    "\n",
    "# define a user-defined function to check if a string is in English\n",
    "def is_english(text):\n",
    "    if text is None:\n",
    "        return False\n",
    "    try:\n",
    "        text.encode('ascii')\n",
    "    except UnicodeEncodeError:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "# register the UDF\n",
    "is_english_udf = udf(is_english, BooleanType())\n",
    "\n",
    "# filter out non-English text\n",
    "df_filtered = df.filter(is_english_udf(df[\"text\"]))\n",
    "\n",
    "# For aggregating data over a specified window of time, you can use the window function\n",
    "# Here, we use a 1-hour window, sliding every 30 minutes\n",
    "window_duration = \"1 hour\"\n",
    "slide_duration = \"30 minutes\"\n",
    "\n",
    "likes_by_location_windowed = (\n",
    "    df_filtered\n",
    "    .groupBy(\n",
    "        window(col(\"timestamp\"), window_duration, slide_duration),\n",
    "        col(\"location\")\n",
    "    )\n",
    "    .agg(sum(\"likes\").alias(\"total_likes\"))\n",
    ")\n",
    "\n",
    "# Start the streaming query and continuously output results to the console\n",
    "query = (\n",
    "    likes_by_location_windowed\n",
    "    .writeStream\n",
    "    .outputMode(\"complete\")\n",
    "    .format(\"console\")\n",
    "    .option(\"truncate\", \"false\")\n",
    "    .trigger(processingTime=\"30 seconds\")\n",
    "    .start()\n",
    ")\n",
    "\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d37223",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}